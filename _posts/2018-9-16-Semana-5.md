---
layout: post
mathjax: true
title: RecSys Semana 5
---

## Comentario: Content-based recommendation systems [In The adaptive web](https://pdfs.semanticscholar.org/3444/6adc7d701a2c3a89c2fc5f6d3479eef407b0.pdf)

### Resumen

Los recomendadores basados en contenido recomiendan items a usuarios según la descripción del item y lo que saben del usuario. 
Obtener información util del contenido del item es ma fácil de minar en el caso de datos semi-estructurados (algunos campos restringidos) que para casos de datos no-estructurados (como campos de texto sin restricciones). 
Para generar una representación estructurada de estos datos no estructurados pueden usarse técnicas como *stemming* (obtener raíces de palabras) o **TF-IDF** para obtener un termino que indique la importancia relativa de cada termino en el texto. 
TF-IDF no captura el contexto de las palabras porque solo considera el numero de veces que aparece pero obtiene buenos resultados.

Los perfiles de usuario contienen la representación de los gustos de los usuarios en *perfiles*. 
Estos pueden contener un modelo de las preferencias de los usuarios o un historial de interacciones anteriores (que luego pueden ser usadas por un *rule-based system*). 
En el primer caso el modelo puede construirse con colaboración del usuario (donde cada uno declara sus preferencias), o pueden aprenderse usando clasificadores binarios (*¿gusta o no gusta?*).

Entre los clasificadores que sirven para determinar la preferencia de los usuarios por un cierto item estan:

* Arboles de decision e inducción de reglas
* Vecinos Cercanos (basado en similaridad entre elementos y otros puntuados por el usuario)
* Algoritmo de Rocchio (para refinar las *queries* incrementalmente).
* Clasificadores lineares
* Naive Bayes (cualquiera de sus variantes)


### Comentarios

Encontré que al paper le faltó considerar otros tipos de items para los que TF-IDF no sirve para extraer características suficientes como lo son videos o imágenes.
Como sale en el texto los dos pasos principales para hacer recomendación basado en contenido es obtener representaciones del usuario y **del contenido**. 
La mayoría del contenido consumido hoy en día no es escrito por lo que se vuelve un poco irrelevante el paper para personas el 2018 (fue escrito en 2007).
Aprovecho de comentar sobre aplicaciones de redes neuronales al area dado que me he informado del tema:

* modelos basados en el paradigma encoder-decoder permiten obtener representaciones vectoriales del contenido independiente de la estructura de este (ej. *autoencoder* de imágenes).
* representaciones de textos que incluyen contexto pueden ser obtenidas usando embeddings de palabras obtenidas usando *GloVe*, **W2V**, **skip-thoughts** entre otras. Los embeddings pueden luego ser usadas en **Doc2Vec** o otro modelo que use un encoder del contenido (ej. una **RNN** que genere resúmenes del texto debiera poder encontrar representaciones útiles de los textos).
* Las técnicas de modelamiento de secuencias que son validas para texto lo son para videos (u otras secuencias), lo que permite generar representaciones de estos (se puede usar el encoder de [esto](https://arxiv.org/abs/1804.00819)).

Por ultimo quiero comentar que la razon por la que no importa para este paper que TF-IDF pierda informacion contextual es que todos los algoritmos mencionados asumen independencia entre cada dimension en el espacio de factores.
Una representación que sería compatible con estos modelos sin perder tanta información contextual podría usar **N-Grams** para generar tokens por pares de palabras.
Luego podríamos filtrar usando el IDF para eliminar las palabras (o n-gramas) que aparecen en todos los documentos.





