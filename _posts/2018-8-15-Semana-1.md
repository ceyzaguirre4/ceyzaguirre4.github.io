---
layout: post
title: RecSys Semana 1
---

## Comentario: How not to sort by Average Rating, [Evan Miller Blog](http://www.evanmiller.org/how-not-to-sort-by-average-rating.html)


### Problema

EL articulo de Blog trata sobre recomendacion de articulos usando opiniones de usuarios en forma de *likes* (o *dislikes*). 
Se escapa del clasico paradigma de sistemas recomendadores que hemos visto hasta el momento donde las valoraciones existen como rankings.
El autor critica la forma de recomendar que utilizan algunas paginas y las critica, para luego sugerir una forma de recomendar que sea robusta a las faltas de las mencionadas.

Para ordenar por preferencia (y asi poder recomendar) algunas paginas calculan el *puntaje* del *item* restando a las valoraciones positivas las negativas. 
El problema que esto tiene es que aquellas paginas con muchas valoraciones pueden tener un gran *puntaje* aunque tengan casi igual proporcion de valoraciones positivas vs negativas (ie, ariticulo mediocre).

Otras paginas evitan el problema calculando el procentaje como la proporcion de valoraciones positivas respecto al total. 
Esta construccion conserva la informacion que nos intersa pero falla cuando un *item* tiene pocas valoraciones.
Un ejemplo representativo de esto es un articulo con 100% de aprobacion que solo ha sido puntuado por su autor (el mitico *autolike*).
Se entiende por lo mismo que mientras no hayan suficientes valoraciones es poco creible que la proporcion sea un buen aproximador de la calidad del *item*.

### Solucion Propuesta

Dado que estabamos hablando de la confianza que tenemos en que proporcion estime correctamente la calidad del *item* resulta razonable que usemos intervalos de confianza. 
Mas especificamente, en que usemos la cota inferior del intervalo de confianza (el minimo valor que podria tomar la verdadera calidad medido en proporcion de *likes*). 
Asi, usando una confianza de 0.95 podemos afirmar con 95% de certeza que la puntuacion es mayor o igual a la cota inferior.

### Discusion

#### Negativos

* En primer lugar, una debilidad de usar este sistema es que esconde los elementos nuevos.
Esto sucede porque mientras menor sea el total de valoraciones mas extremos son los valores de las cotas respecto a la media.
En otras palabras independiente de lo bueno que sea un *item*, sera poco recomendado mientras no adquiera una masa critica de valoraciones.
Esto establece un circulo vicioso porque es poco probable que suficientes personas voten por el articulo si este no es recomendado.

* Construccion provista no es compatible con otras formas de puntuar (ej, x estrellas de 5) sin conversiones previas entre los formatos.

#### Positivos

* Usar Wilson Score combina Most popular (usando como proxy de popularidad el numero de valoraciones) con los valores reales de los ratings para sugerir mejor en escenarios donde un item tiene pocas valoraciones.

* Usar intervalos de confianza hacen que el sistema sea mas robusto a ataques que serian graves si solo se usara la proporcion como metrica. 
Por ejemplo, si un grupo de personas (o un individuo con bots) deciden puntuar mal el *item* injustamente.

### Extensiones posibles

El concepto estadistico de intervalos de confianza no esta restringido a *likes* o *deslikes*. Puede aplicarse el mismo concepto para predecir la cota inferior del umbral de confianza para otras metricas como lo son *x estrellas de 5* o un valor cualquiera de un dominio continuo.

Si asumimos que la distribucion de datos es normal y que los datos son aleatorios e independientes podemos calcular la cota inferior del intervalo de confianza para el promedio de las valoraciones. 
Este valor representara la puntuacion minima que podria tomar la verdadera calidad de un articulo y se calcula usando el promedio de valoraciones como estimador de calidad.


<img src="http://www.ub.edu/stat/GrupsInnovacio/Statmedia/demo/Temas/Capitulo8/Images/normal5.gif" style="width: 250px; display:block; margin:auto"/>

## Comentario: Collaborative filtering recommender systems, [In The adaptive web ](https://pdfs.semanticscholar.org/d17d/3fa8083c4de1f5545446a1f59da54a1dba21.pdf)


### Resumen

El *paper*  es un survey de sistemas colaborativos que definen como el filtrado o evaluacion de *items* usando las opiniones de otras personas. 
Al entender de los autores la base para este filtrado tiene sus origenes en el *boca a boca* y con el advenimiento de los computadores y la web se abre la posibilidad de analizar fuentes masivas de datos. 
Dado que inteligencias artificiales que automaticen el proceso no existen (al momento de escribirse el *paper*) se utilizan jucios humanos en la forma de ratings para avanzar el estado del arte. 
Entendemos que los *ratings* pueden tomar una variedad de formas (escalar, binario, unario) y pueden ser recolectados explicitamente o implicitamente.
La importancia de esta forma de filtrado en el contexto de la web (contexto que envuelve el articulo) es obvio, adaptar el contenido a las necesidades de cada usuario.

Para lograr su objetivo los diseñadores deben primero identificar las tareas que el usuario quiere automatizadas en el sistema (encontrar nuevos *item*s para mi/mi grupo, encontrar personas con gustos parecidos, aconsejar sobre un producto especifico, etc.).
En segundo lugar deben identificar las funcionalidades que se ofreceran (recomendar *item*s, puntuar un *item*, busqueda constreñida) y analizar el dominio para ver si es propicio a tecnicas de este estilo segun los datos (muchos items y puntuaciones, muchos usuarios, usuarios puntuan mucho).
Por ultimo se debe analidar la necesidad/validez de comparar usuarios (*items* son distintos en los subjetivo, usuarios tienen gustos agrupables) y la utilidad de las puntuaciones (*items* persisten, los gustos de los usuarios se mantienen).

Una forma alternativa para lograr el mismo objetivo (de realizar recomendaciones) es usar filtrado por contenido.
Los autores comentan sobre la utilidad de estos sistemas para sugerir *items* no puntuados, pero destacan que son mas propensos a la sobre-especializacion (perdiendo novedad y/o serendipia). 
Terminan la comparacion mencionando un *approach* hibrido (combinacion de ambos sea automatica o manual).

Para lograr hacer recomendaciones se necesitan modelos que los autores dividen en *memory-based* (requieren que todos los *ratings*, *items* y *users* esten almacenados en memoria y por tanto no escalan bien en el mundo real) y *model-based* (periodicamente crean un resumen de los patrones existentes). 
El paper tambien explora la distinccion segun si se basan en un modelo probabilistico o no destacando en la primera categoria redes bayesianas.
Para la segunda se resalta *nearest neighbors* en el espacio de usuarios (*rating* es suma ponderada y normalizada de las puntuaciones de personas similares) o en el espacio de *items* (*rating* es suma ponderada de puntuaciones que el usuario hizo a elementos similares encontrados observando puntuaciones de otros usuarios).
Las tareas puede hacerse mas eficiente usando tecnicas para no considerar todos los usuarios (considerar los k vecinos mas cercanos, *subsampling*, *clustering*) para el primero, y pruning y/o reduccion de dimensionalidad para el segundo.
Por ultimo (para los no-probabilisticos) se considera la mineria de reglas asociativas (*association rule mining*) pero se desecha por su ineficiencia en el dominio de filtrado colaborativo.

Independiente del algoritmo elegido los autores nombran problemas generalmente ignorados que los algoritmos tienen que superar como cuando hay pocos ratings (soluciones para estos casos: ignorar, ajustar calculos, incorporar un conocimiento previo), la distinccion entre prediccion y recomendacion (solo para el primer caso el sistema tiene que conocer todos los *items*), y hacer explicita la confianza del algoritmo en la decision que tomo.

Sobre la adquisicion de *ratings* se explica que aquellos obtenidos explicitamente ofrecen una descripcion mas precisa de las preferencias de los usuarios, pero son mas dificiles de recolectar (aunque estudios muestran que es mas facil de lo inicialmente esperado por beneficios sociales y practicos). 
Por otro lado, aquellos obtenidos *implicitamente* (obsercando el comportamiento del usuario) pueden ser imprecisos pero una vez que se obtienen en cantidades suficientes disminuye la incertidumbre dado que se pueden acumular (por votacion o promedio) y la incertidumbre se reduce por agregacion.

Los *ratings* adquiridos son usualmente usados luego para evaluar el rendimiento del sistema pero no hay una metrica aceptada por todos.
La mas frecuente es *accuracy* que mide la capacidad de un sistema de predecir la puntuacion que un usuario dara a un *item* (*predictive accuracy*) o la utilidad de una lista recomendada para un cierto usuario (*rank accuracy*).
Otras metricas usadas miden *novelty/serendipity* (capacidad para recomendar *items* desconocidos), *coverage* (cobertura, porcentaje de *items* potencialmente recomendables), *learning rate* (cuan rapido se vuelve efectivo), *confidence* (seguridad de prediccion), además de otros criterios como la satisfaccion de los usuarios o analisis de trafico/compras del sistema.

<!--Son importantes estas metricas alternativas porque *accuracy* no considera todas las necesidades reales del usuario como lo son la facilidad de uso, ... social navigation ... explanation -->

Por ultimo es importante reconocer la importancia del uso y almacenamiento de los datos recolectados que aseguren la privacidad del usuario para mantener la integridad de esta informacion.
Tambien es importante resguardar el servicio de aquellos que intenten romperlo usando los *ratings* en otro modo que el esperado.

### Comentarios

* Si bien se nombra el temor del usuario hacia el mal uso de los datos el escrito considera todo desde la confianza del usuario y los costos/beneficios para el sistema.
El uso etico de los datos por el bien del usuario se ha hecho mas relevante en el ultimo año (2018) y aprovecho de destacarlo por lo mismo.

* El *paper* no puede seguir siendo considerado un *comprehensive survey* del estado del arte por su edad. 
Entre los temas no se tratan destaca el uso de redes neuronales que dirigen el estado del arte actualmente. 
